import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load the dataset

data = pd.read_csv("Test Dataset.csv")

# Data Cleaning: Handling missing values
data['Gender'].fillna(data['Gender'].mode()[0], inplace=True)
data['Married'].fillna(data['Married'].mode()[0], inplace=True)
data['Dependents'].fillna(data['Dependents'].mode()[0], inplace=True)
data['Self_Employed'].fillna(data['Self_Employed'].mode()[0], inplace=True)
data['LoanAmount'].fillna(data['LoanAmount'].median(), inplace=True)
data['Loan_Amount_Term'].fillna(data['Loan_Amount_Term'].mode()[0], inplace=True)
data['Credit_History'].fillna(data['Credit_History'].mode()[0], inplace=True)

# Feature Engineering: Transforming 'Dependents' column
data['Dependents'] = data['Dependents'].replace('3+', 3).astype(int)

# Dropping 'Loan_ID' as it is not relevant for prediction
data.drop('Loan_ID', axis=1, inplace=True)

# Simulate the target variable 'Loan_Status' for demonstration
# For actual projects, ensure you have the real target variable
np.random.seed(42)
data['Loan_Status'] = np.random.choice([0, 1], size=len(data))

# Data Preprocessing: Encoding categorical variables
categorical_cols = ['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area']
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    label_encoders[col] = le

# Feature Scaling: Scaling numerical variables
scaler = StandardScaler()
numerical_cols = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']
data[numerical_cols] = scaler.fit_transform(data[numerical_cols])

# Splitting the data into features and target variable
X = data.drop('Loan_Status', axis=1)
y = data['Loan_Status']

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Model Training: Using Random Forest classifier
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Model Evaluation: Making predictions and evaluating the model
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# Revert scaling for better visualization
data[numerical_cols] = scaler.inverse_transform(data[numerical_cols])

# Distribution of Numerical Features
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
for i, col in enumerate(numerical_cols):
    sns.histplot(data[col], kde=True, ax=axes[i//2, i%2])
    axes[i//2, i%2].set_title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

# Count Plot for Categorical Features
fig, axes = plt.subplots(3, 2, figsize=(15, 15))
for i, col in enumerate(categorical_cols):
    sns.countplot(x=data[col], ax=axes[i//2, i%2])
    axes[i//2, i%2].set_title(f'Count Plot of {col}')
    axes[i//2, i%2].set_xticklabels(label_encoders[col].classes_)
axes[2, 1].axis('off')
plt.tight_layout()
plt.show()

# Feature Importance Plot
feature_importances = model.feature_importances_
features = X.columns
indices = np.argsort(feature_importances)[::-1]

plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importances[indices], y=features[indices])
plt.title('Feature Importance')
plt.show()

# Confusion Matrix Plot
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Displaying the results
print(f'Accuracy: {accuracy}')
print('Classification Report:')
print(report)
print('Confusion Matrix:')
print(conf_matrix)
